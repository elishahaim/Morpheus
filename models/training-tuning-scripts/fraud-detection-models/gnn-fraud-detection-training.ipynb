{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud detection pipeline using Graph Neural Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content\n",
    "1. Introduction\n",
    "1. Load transaction data\n",
    "2. Graph construction\n",
    "3. GraphSage training\n",
    "5. Classifiction and Prediction\n",
    "6. Evaluation\n",
    "7. Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction\n",
    "This workflow shows an application of a graph neural network for fraud detection in a credit card transaction graph. We use a transaction dataset that includes three types of nodes, `transaction`, `client`, and `merchant` nodes. We use `GraphSAGE` along `XGBoost` to identify frauds in transactions. Since the graph is heterogeneous we employ HinSAGE a heterogeneous implementation of GraphSAGE.\n",
    "\n",
    "First, GraphSAGE is trained separately to produce embedding of transaction nodes, then the embedding is fed to `XGBoost` classifier to identify fraud and nonfraud transactions. During the inference stage,  an embedding for a new transaction is computed from the trained GraphSAGE model and then feed to XGBoost model to get the anomaly scores."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Loading the Credit Card Transaction Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import dgl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from model import HeteroRGCN\n",
    "from model import HinSAGE\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from torchmetrics.functional import accuracy\n",
    "from tqdm import trange\n",
    "from xgboost import XGBClassifier\n",
    "from training import (get_metrics, evaluate, init_loaders, build_fsi_graph,\n",
    "                     map_node_id, prepare_data, save_model, train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1001)\n",
    "torch.manual_seed(1001)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#device "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load traing and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace training-data.csv and validation-data.csv with training & validation csv in dataset file.\n",
    "TRAINING_DATA ='../../datasets/training-data/fraud-detection-training-data.csv'\n",
    "VALIDATION_DATA = '../../datasets/validation-data/fraud-detection-validation-data.csv'\n",
    "train_data = pd.read_csv(TRAINING_DATA)\n",
    "inductive_data = pd.read_csv(VALIDATION_DATA)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the number of samples of training data is small we augment data using benign transaction examples from the original training samples. This increases the number of benign example and reduce the proportion of fraudulent transactions. This is similar to practical situation where frauds are few in proportion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase number of samples.\n",
    "def augement_data(train_data=train_data, n=20):\n",
    "    max_id = inductive_data.index.max()\n",
    "    non_fraud = train_data[train_data['fraud_label'] == 0]\n",
    "    \n",
    "    non_fraud = non_fraud.drop(['index'], axis=1)\n",
    "    df_fraud = pd.concat([non_fraud for i in range(n)])\n",
    "    df_fraud.index = np.arange(1076, 1076 + df_fraud.shape[0])\n",
    "    df_fraud['index'] = df_fraud.index\n",
    "    \n",
    "    return pd.concat((train_data, df_fraud))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = augement_data(train_data, n=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `train_data` variable stores the data that will be used to construct graphs on which the representation learners can train. \n",
    "The `inductive_data` will be used to test the inductive performance of our representation learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The distribution of fraud for the train data is:\n",
      " 0    11865\n",
      "1      188\n",
      "Name: fraud_label, dtype: int64\n",
      "The distribution of fraud for the inductive data is:\n",
      " 0    244\n",
      "1     21\n",
      "Name: fraud_label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('The distribution of fraud for the train data is:\\n', train_data['fraud_label'].value_counts())\n",
    "print('The distribution of fraud for the inductive data is:\\n', inductive_data['fraud_label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train, test and create nodes index\n",
    "def prepare_data(df_train, df_test):\n",
    "    \n",
    "    train_idx_ = df_train.shape[0]\n",
    "    df = pd.concat([df_train, df_test], axis=0)\n",
    "    df['tran_id'] = df['index']\n",
    "\n",
    "    meta_cols = ['tran_id', 'client_node', 'merchant_node']\n",
    "    for col in meta_cols:\n",
    "        map_node_id(df, col)\n",
    "\n",
    "    train_idx = df['tran_id'][:train_idx_]\n",
    "    test_idx = df['tran_id'][train_idx_:]\n",
    "\n",
    "    df['index'] = df['tran_id']\n",
    "    df.index = df['index']\n",
    "\n",
    "    return (df.iloc[train_idx, :], df.iloc[test_idx, :], train_idx, test_idx, df['fraud_label'].values, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_idx, inductive_idx, labels, df = prepare_data(train_data, inductive_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Construct transasction graph network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, nodes, edges, and features are passed to the `build_fsi_graph` method. Note that client and merchant node data are featurless, instead node embedding is used as a feature for these nodes. Therefore all the relevant transaction data resides at the transaction node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_cols = [\"client_node\", \"merchant_node\", \"fraud_label\", \"index\", \"tran_id\"]\n",
    "\n",
    "# Build graph\n",
    "whole_graph, feature_tensors = build_fsi_graph(df, meta_cols)\n",
    "train_graph, _ = build_fsi_graph(train_data, meta_cols)\n",
    "whole_graph = whole_graph.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset to tensors\n",
    "feature_tensors = feature_tensors.to(device)\n",
    "train_idx = torch.from_numpy(train_idx.values).to(device)\n",
    "inductive_idx = torch.from_numpy(inductive_idx.values).to(device)\n",
    "labels = torch.LongTensor(labels).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes={'client': 623, 'merchant': 388, 'transaction': 12053},\n",
      "      num_edges={('client', 'buy', 'transaction'): 12053, ('merchant', 'sell', 'transaction'): 12053, ('transaction', 'bought', 'client'): 12053, ('transaction', 'issued', 'merchant'): 12053},\n",
      "      metagraph=[('client', 'transaction', 'buy'), ('transaction', 'client', 'bought'), ('transaction', 'merchant', 'issued'), ('merchant', 'transaction', 'sell')])\n"
     ]
    }
   ],
   "source": [
    "# Show structure of training graph.\n",
    "print(train_graph)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train Heterogeneous GraphSAGE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HinSAGE, a heterogeneous graph implementation of the GraphSAGE framework is trained with user specified hyperparameters. The model train several GraphSAGE models on the type of relationship between different types of nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "target_node = \"transaction\"\n",
    "epochs = 20\n",
    "in_size, hidden_size, out_size, n_layers,\\\n",
    "    embedding_size = 111, 64, 2, 2, 1\n",
    "batch_size = 100\n",
    "hyperparameters = {\"in_size\": in_size, \"hidden_size\": hidden_size,\n",
    "                   \"out_size\": out_size, \"n_layers\": n_layers,\n",
    "                   \"embedding_size\": embedding_size,\n",
    "                   \"target_node\": target_node,\n",
    "                   \"epoch\": epochs}\n",
    "\n",
    "\n",
    "scale_pos_weight = train_data['fraud_label'].sum() / train_data.shape[0]\n",
    "scale_pos_weight = torch.tensor(\n",
    "    [scale_pos_weight, 1-scale_pos_weight]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloaders\n",
    "train_loader, val_loader, test_loader = init_loaders(train_graph.to(\n",
    "    device), train_idx, test_idx=inductive_idx,\n",
    "    val_idx=inductive_idx, g_test=whole_graph, batch_size=batch_size)\n",
    "\n",
    "\n",
    "# Set model variables\n",
    "model = HinSAGE(train_graph, in_size, hidden_size, out_size, n_layers, embedding_size).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "loss_func = nn.CrossEntropyLoss(weight=scale_pos_weight.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:02<00:47,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/20 | Train Accuracy: 1.0 | Train Loss: 4.077046836914391\n",
      "Validation Accuracy: 0.9207547307014465 auc 0.13992974238875877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [00:04<00:43,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 | Train Accuracy: 1.0 | Train Loss: 110.9858230000423\n",
      "Validation Accuracy: 0.9207547307014465 auc 0.5852849336455894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [00:07<00:40,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 | Train Accuracy: 1.0 | Train Loss: 419.0077720507543\n",
      "Validation Accuracy: 0.9207547307014465 auc 0.6083138173302107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [00:09<00:37,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 | Train Accuracy: 1.0 | Train Loss: 176.4732639742433\n",
      "Validation Accuracy: 0.9169811606407166 auc 0.7976190476190476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [00:11<00:34,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 | Train Accuracy: 1.0 | Train Loss: 49.66766470632865\n",
      "Validation Accuracy: 0.9245283007621765 auc 0.8080601092896175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [00:14<00:33,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 | Train Accuracy: 1.0 | Train Loss: 31.406425931840204\n",
      "Validation Accuracy: 0.9283018708229065 auc 0.858216237314598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [00:16<00:30,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 | Train Accuracy: 1.0 | Train Loss: 24.368114110082388\n",
      "Validation Accuracy: 0.9283018708229065 auc 0.8635831381733021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [00:18<00:27,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 | Train Accuracy: 1.0 | Train Loss: 17.363841364858672\n",
      "Validation Accuracy: 0.9283018708229065 auc 0.8762685402029665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [00:21<00:25,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 | Train Accuracy: 1.0 | Train Loss: 16.201855568680912\n",
      "Validation Accuracy: 0.9320755004882812 auc 0.8788056206088993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [00:23<00:23,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 | Train Accuracy: 1.0 | Train Loss: 15.001215729862452\n",
      "Validation Accuracy: 0.9320755004882812 auc 0.8873926619828258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [00:25<00:21,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 | Train Accuracy: 1.0 | Train Loss: 14.861962082330137\n",
      "Validation Accuracy: 0.9358490705490112 auc 0.8791959406713505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [00:28<00:19,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 | Train Accuracy: 1.0 | Train Loss: 13.089418702758849\n",
      "Validation Accuracy: 0.9320755004882812 auc 0.8858313817330211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [00:30<00:16,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 | Train Accuracy: 1.0 | Train Loss: 12.216756469802931\n",
      "Validation Accuracy: 0.9320755004882812 auc 0.9127634660421545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [00:33<00:14,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 | Train Accuracy: 1.0 | Train Loss: 12.858742844546214\n",
      "Validation Accuracy: 0.9433962106704712 auc 0.9182279469164715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [00:35<00:12,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 | Train Accuracy: 1.0 | Train Loss: 11.10123936785385\n",
      "Validation Accuracy: 0.9320755004882812 auc 0.911592505854801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [00:38<00:09,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 | Train Accuracy: 1.0 | Train Loss: 15.444379360007588\n",
      "Validation Accuracy: 0.9207547307014465 auc 0.8721701795472288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [00:40<00:07,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 | Train Accuracy: 1.0 | Train Loss: 15.353719354665373\n",
      "Validation Accuracy: 0.9169811606407166 auc 0.822599531615925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [00:42<00:04,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 | Train Accuracy: 1.0 | Train Loss: 15.88208947563544\n",
      "Validation Accuracy: 0.9283018708229065 auc 0.8528493364558939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [00:45<00:02,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 | Train Accuracy: 1.0 | Train Loss: 12.539632054162212\n",
      "Validation Accuracy: 0.9283018708229065 auc 0.9022248243559718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:47<00:00,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 | Train Accuracy: 1.0 | Train Loss: 13.172684742690763\n",
      "Validation Accuracy: 0.9433962106704712 auc 0.9342310694769711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in trange(epochs):\n",
    "\n",
    "    train_acc, loss = train(\n",
    "        model, loss_func, train_loader, labels, optimizer, feature_tensors,\n",
    "        target_node, device=device)\n",
    "    print(f\"Epoch {epoch}/{epochs} | Train Accuracy: {train_acc} | Train Loss: {loss}\")\n",
    "    val_logits, val_seed, _ = evaluate(model, val_loader, feature_tensors, target_node, device=device)\n",
    "    val_accuracy = accuracy(val_logits.argmax(1), labels.long()[val_seed].cpu(), \"binary\").item()\n",
    "    val_auc = roc_auc_score(\n",
    "        labels.long()[val_seed].cpu().numpy(),\n",
    "        val_logits[:, 1].numpy(),\n",
    "    )\n",
    "    print(f\"Validation Accuracy: {val_accuracy} auc {val_auc}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Inductive Step GraphSAGE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we want to compute the inductive embedding of a new transaction. To extract the embedding of the new transactions, we need to keep indices of the original graph nodes along with the new transaction nodes. We need to concatenate the test data frame to the train data frame to create a new graph that includes all nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes={'client': 861, 'merchant': 482, 'transaction': 12318},\n",
      "      num_edges={('client', 'buy', 'transaction'): 12318, ('merchant', 'sell', 'transaction'): 12318, ('transaction', 'bought', 'client'): 12318, ('transaction', 'issued', 'merchant'): 12318},\n",
      "      metagraph=[('client', 'transaction', 'buy'), ('transaction', 'client', 'bought'), ('transaction', 'merchant', 'issued'), ('merchant', 'transaction', 'sell')])\n"
     ]
    }
   ],
   "source": [
    "print(whole_graph)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inductive step applies the previously learned (and optimized) aggregation functions, part of the `trained_hinsage_model`. We also pass the new graph g_test, test data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy: 0.9320755004882812 auc 0.889344262295082\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings\n",
    "_, train_seeds, train_embedding = evaluate(model, train_loader, feature_tensors, target_node, device=device)\n",
    "test_logits, test_seeds, test_embedding = evaluate(model, test_loader, feature_tensors, target_node, device=device)\n",
    "\n",
    "# compute metrics\n",
    "test_acc = accuracy(test_logits.argmax(dim=1), labels.long()[test_seeds].cpu(), \"binary\").item()\n",
    "test_auc = roc_auc_score(labels.long()[test_seeds].cpu().numpy(), test_logits[:, 1].numpy())\n",
    "\n",
    "metrics_result = pd.DataFrame()\n",
    "print(f\"Final Test Accuracy: {test_acc} auc {test_auc}\")\n",
    "\n",
    "#acc, f_1, precision, recall, roc_auc, pr_auc, average_precision, _, _ = get_metrics(\n",
    "#    test_logits.numpy(), labels[test_seeds].cpu().numpy())\n",
    "\n",
    "#print(f\"Final Test Accuracy: {acc} auc {roc_auc}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Classification: predictions based on inductive embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now a selected classifier (XGBoost) can be trained using the training node embedding and test on the test node embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=None, ...)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train XGBoost classifier on embedding vector\n",
    "classifier = XGBClassifier(n_estimators=100)\n",
    "classifier.fit(train_embedding.cpu().numpy(), labels[train_seeds].cpu().numpy())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If requested, the original transaction features are added to the generated embeddings. If these features are added, a baseline consisting of only these features (without embeddings) is included to analyze the net impact of embeddings on the predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_pred = classifier.predict_proba(test_embedding.cpu().numpy())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the highly imbalanced nature of the dataset, we can evaluate the results based on AUC, accuracy ...etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy: 0.9245283018867925 auc 0.9055425448868072\n"
     ]
    }
   ],
   "source": [
    "acc, f_1, precision, recall, roc_auc, pr_auc, average_precision, _, _ = get_metrics(\n",
    "    xgb_pred, labels[inductive_idx].cpu().numpy(),  name='HinSAGE_XGB')\n",
    "print(f\"Final Test Accuracy: {acc} auc {roc_auc}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result shows, using GNN embedded features with XGB achieves with a better performance when tested over embedded features. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Save models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graphsage and xgboost models can be saved into their respective save format using `save_model` method. For infernce, graphsage load as pytorch model, and the XGBoost load using `cuml` *Forest Inference Library (FIL)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir= \"modelpath/\"\n",
    "\n",
    "save_model(train_graph, model, hyperparameters, classifier, model_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph.pkl  hyperparams.pkl  model.pt  xgb.pt\n"
     ]
    }
   ],
   "source": [
    "!ls modelpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For inference we can load from file as follows. \n",
    "from training import load_model\n",
    "# do inference on loaded model, as follows\n",
    "# hinsage_model,  hyperparam, g = load_model(model_dir, device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this workflow, we show a hybrid approach how to use Graph Neural network along XGBoost for a fraud detection on credit card transaction network. For further, optimized inference pipeline refer to `Morpheus` inference pipeline of fraud detection."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "1. Van Belle, Rafaël, et al. \"Inductive Graph Representation Learning for fraud detection.\" Expert Systems with Applications (2022): 116463.\n",
    "2.https://stellargraph.readthedocs.io/en/stable/hinsage.html?highlight=hinsage\n",
    "3.https://github.com/rapidsai/clx/blob/branch-0.20/examples/forest_inference/xgboost_training.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
